{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture:\n",
    "\n",
    "What can we do now. For one year, we can grab the products we want, and plot their price over the year. Now all all wee need to to is repeat this process over a bunch of years  (we will do 2007-2010) and make a nice plot. That's it. \n",
    "\n",
    "Now the details...\n",
    "\n",
    "Below are some stuff that I sorted out for you. \n",
    "\n",
    "Here is the product list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\"BUD LIGHT\", \"MILLER LITE\", \"COORS LIGHT\", \"CORONA EXTRA\", \"HEINEKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the file list. So given how I gave the files to you, this will allow you to grab the files that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"year_2008\\\\beer\\\\beer_groc_1479_1530\", \"year_2009\\\\beer\\\\beer_groc_1531_1582\",\n",
    "             \"year_20010\\\\beer\\\\beer_groc_1583_1634\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a function that we will use to convert the week into a date-time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_time(week):\n",
    "    '''\n",
    "    converts excel float format to pandas datetime object\n",
    "    round to '1min' with \n",
    "    .dt.round('1min') to correct floating point conversion innaccuracy\n",
    "    '''\n",
    "    \n",
    "    excel_time = (week-400)*7 + 31900-6\n",
    "    \n",
    "    return pd.to_datetime('1899-12-30') + pd.to_timedelta(excel_time, unit = \"D\", box = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick aside on reading in the data.\n",
    "\n",
    "One the github repository there is a notebook [https://github.com/mwaugh0328/data_bootcamp_spring_2019/blob/master/guided_project/parquet_format.ipynb] which walks through how to convert the scanner datesets into a format that will read in much faster. For example, on my laptop, reading in the scanner dataset takes five minutes. After converting it to the `parquet` format, it will read in in 5 seconds. Huge speed up. \n",
    "\n",
    "Because we are going to have to read in the scanner data sets 4 times, you might find converting it first easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. First do this for year 2007\n",
    "\n",
    "What are the steps:\n",
    "\n",
    "- Read in the product file for 2007\n",
    "\n",
    "- Read in the scanner file for 2007\n",
    "\n",
    "- For each item in the product list, generate the UPCS, and then grab the transactions.\n",
    "\n",
    "- Create a new column in the dataframe called ``date``. Create this column by applying the function `convert_excel_time` to the ``WEEK`` variable from the transaction. The `date` column should be a datetime object. Verify this. \n",
    "\n",
    "- Given the file with all the relevant transactions ``groupby`` on brand and `date`. Then use ``.agg``  to aggregate units and dollars by sum. Then create a price by dividing the aggregated dollars by units. \n",
    "\n",
    "Call this resulting dataframe ``all_beer`` we will use it below when we work through the 2008-2010 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Now do this same thing for all years after 2007.\n",
    "\n",
    "- Step one is to read in the **2008 product file** this is important! This is the same product file for 2008 and later. \n",
    "\n",
    "- Then we want to create a ``for loop`` to work through each of the operations as we did above in Step 1. for each scanner file. Unless you organized your files differently, use the file list that I provided above to execute this loop. \n",
    "\n",
    "- Let's recap the steps within the ``for loop``. \n",
    "\n",
    "\n",
    " 1. Step one, given the file, read in scanner data set.\n",
    "\n",
    "\n",
    " 2. Create an empty data frame. For each product grab the upcs, grab the transactions and fill in the empty dataframe. \n",
    "\n",
    "\n",
    " 3. Create a new column in the dataframe called ``date``. Create this column by applying the function `convert_excel_time` to the ``WEEK`` variable from the transaction. The `date` column should be a datetime object. Verify this. \n",
    "\n",
    "\n",
    " 4. At this point you should have a dataframe that has all the transactions that we want for the year. Then we perform a ``groupby`` on ``brand`` and ``date``, aggregate the units and dollars using the sum, Then construct a price (units/dollars)\n",
    "\n",
    "\n",
    " - **Important final step** append that file to to ``all_beer`` file described above. Then proceed to the next file, etc. untill we have everything we want from 2007-2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Make an awesome plot\n",
    "\n",
    "I'll leave this up to you. Many of you saw what I did. A couple of things you may want to consider.\n",
    "\n",
    "\n",
    "- Have a side by side plot. One is the group of Bud Light, Miller Lite, and Coors. The other is the \"control\" group of Corona and Heiniken. So mimic the stuff in the Miller-Weinberg paper on page 8. \n",
    "\n",
    "\n",
    "- Plot the log price rather than the dollar value. So use the `numpy` function `np.log` to create the log price. \n",
    "\n",
    "\n",
    "- Indicate when the merger took place. This was in June 2008. Note by using the `datetime` package we can just pass the date there and it will find the right place on the x-axis. \n",
    "\n",
    "\n",
    "- I would strongly consider using the `resample` function we discussed in the `datetime` lecture. In particular, the data is at the weekly frequency,  plotting it at the monthly frequency. \n",
    "\n",
    "\n",
    "- Colors. I found this website which is cool [https://brandpalettes.com/](https://brandpalettes.com/) so type in the brands and it provides the colors associated with it.\n",
    "\n",
    "\n",
    "- Indicate average values before and after.\n",
    "\n",
    "\n",
    "- Anything else you can dream up!\n",
    "\n",
    "Congrats you made it! Now you can publish a paper in Econometrica!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
